{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładowanie modelu GPT-2 i tokenizera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\" \n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homoglify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMOGLYPHS = {\n",
    "    # Cyrylica (małe litery)\n",
    "    'a': 'а',  # Cyrylica 'a'\n",
    "    'e': 'е',  # Cyrylica 'e'\n",
    "    'o': 'о',  # Cyrylica 'o'\n",
    "    'p': 'р',  # Cyrylica 'p'\n",
    "    'c': 'с',  # Cyrylica 'c'\n",
    "    'x': 'х',  # Cyrylica 'x'\n",
    "    'y': 'у',  # Cyrylica 'y'\n",
    "\n",
    "    # Grecki (wielkie litery)\n",
    "    'A': 'Α',  # Grecka 'A'\n",
    "    'B': 'Β',  # Grecka 'B'\n",
    "    'E': 'Ε',  # Grecka 'E'\n",
    "    'H': 'Η',  # Grecka 'H'\n",
    "    'I': 'Ι',  # Grecka 'I'\n",
    "    'K': 'Κ',  # Grecka 'K'\n",
    "    'M': 'Μ',  # Grecka 'M'\n",
    "    'N': 'Ν',  # Grecka 'N'\n",
    "    'O': 'Ο',  # Grecka 'O'\n",
    "    'P': 'Ρ',  # Grecka 'P'\n",
    "    'T': 'Τ',  # Grecka 'T'\n",
    "    'X': 'Χ',  # Grecka 'X'\n",
    "    'Y': 'Υ',  # Grecka 'Y'\n",
    "\n",
    "    # Cyrylica (wielkie litery) \n",
    "    'C': 'С',  # Cyrylica 'C'\n",
    "}\n",
    "\n",
    "# Generowanie odwrotnej mapy\n",
    "REVERSE_HOMOGLYPHS = {v: k for k, v in HOMOGLYPHS.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja generująca sensowną kontynuację tekstu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuation_with_gpt2(previous_text, max_length=150):\n",
    "    \"\"\"\n",
    "    Generuje sensowną kontynuację tekstu za pomocą modelu GPT-2.\n",
    "    \"\"\"\n",
    "    # Tokenizacja poprzedniego tekstu\n",
    "    input_ids = tokenizer.encode(previous_text, return_tensors='pt')\n",
    "\n",
    "    # Tworzenie maski uwagi (wszystkie tokeny mają wartość 1, bo nie używamy paddingu)\n",
    "    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)\n",
    "\n",
    "    # Generowanie kontynuacji z uwzględnieniem maski uwagi\n",
    "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7, do_sample=True, pad_token_id=50256)\n",
    "\n",
    "    # Dekodowanie wyników\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja kodująca wiadomość:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_message(text, message):\n",
    "    \"\"\"\n",
    "    Koduje wiadomość w tekście, zamieniając litery na homoglifowe odpowiedniki.\n",
    "    \"\"\"\n",
    "    # Konwersja wiadomości na ciąg binarny\n",
    "    message_binary = ''.join(format(ord(ch), '08b') for ch in message)\n",
    "\n",
    "    # Liczba dostępnych homoglifów w tekście\n",
    "    available_homoglyphs = sum(1 for char in text if char in HOMOGLYPHS)\n",
    "    needed_homoglyphs = len(message_binary)\n",
    "\n",
    "    # Sprawdzenie, czy wystarczy znaków do zakodowania\n",
    "    if needed_homoglyphs > available_homoglyphs:\n",
    "        print(f\"W tekście jest za mało homoglifów: {available_homoglyphs}/{needed_homoglyphs}. Generowanie nowego tekstu.\")\n",
    "        text = generate_continuation_with_gpt2(text)  # Generowanie kontynuacji historii\n",
    "        print(f\"---------------\\nNowy tekst:\\n{text}\\n----------------------\\n\")\n",
    "    else:\n",
    "        print(f\"W tekście jest za wystarczająco homoglifów: {available_homoglyphs}/{needed_homoglyphs}.\")\n",
    "    encoded_text = []\n",
    "    message_index = 0\n",
    "\n",
    "    for char in text:\n",
    "        if message_index < len(message_binary) and char in HOMOGLYPHS:\n",
    "            # Zamieniamy na homoglif, jeśli bit wiadomości to 1\n",
    "            if message_binary[message_index] == '1':\n",
    "                encoded_text.append(HOMOGLYPHS[char])\n",
    "            else:\n",
    "                encoded_text.append(char)\n",
    "            message_index += 1\n",
    "        else:\n",
    "            encoded_text.append(char)\n",
    "    \n",
    "    return ''.join(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dekodowanie wiadomości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_message(encoded_text):\n",
    "    \"\"\"\n",
    "    Dekoduje wiadomość z zakodowanego tekstu.\n",
    "    \"\"\"\n",
    "    binary_message = []\n",
    "    \n",
    "    for char in encoded_text:\n",
    "        if char in REVERSE_HOMOGLYPHS:\n",
    "            binary_message.append('1')\n",
    "        elif char in HOMOGLYPHS:\n",
    "            binary_message.append('0')\n",
    "    \n",
    "    # Debug: Wyświetl binarną wiadomość\n",
    "    # print(f\"Odczytana wiadomość binarna: {''.join(binary_message)}\")\n",
    "    \n",
    "    # Grupujemy po 8 bitów (jeden znak w ASCII) i konwertujemy na tekst\n",
    "    message_chars = [\n",
    "        chr(int(''.join(binary_message[i:i+8]), 2))\n",
    "        for i in range(0, len(binary_message), 8)\n",
    "    ]\n",
    "    return ''.join(message_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wywołanie programu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Once upon a time, there was a brave hero named Alex.\"\n",
    "hidden_message = \"Ela z mlotkiem w Dubaju\" # mniej więcej maksymalna długośc wiadomości\n",
    "# hidden_message = \"Hi\" \n",
    "\n",
    "try:\n",
    "    # Kodowanie\n",
    "    encoded_text = encode_message(original_text, hidden_message)\n",
    "    print(\"Zakodowany tekst:\", encoded_text)\n",
    "\n",
    "    # Dekodowanie\n",
    "    decoded_message = decode_message(encoded_text)\n",
    "    print(\"Odkodowana wiadomość:\", decoded_message)\n",
    "except ValueError as e:\n",
    "    print(f\"Błąd: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
