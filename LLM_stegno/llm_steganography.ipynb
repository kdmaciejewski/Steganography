{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "meta-llama/Meta-Llama-3-8B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001B[0m in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    258\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 259\u001B[1;33m         \u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_for_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    260\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mHTTPError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\requests\\models.py\u001B[0m in \u001B[0;36mraise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    952\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhttp_error_msg\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 953\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhttp_error_msg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    954\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mGatedRepoError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\transformers\\utils\\hub.py\u001B[0m in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001B[0m\n\u001B[0;32m    416\u001B[0m         \u001B[1;31m# Load from URL or cache if already cached\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 417\u001B[1;33m         resolved_file = hf_hub_download(\n\u001B[0m\u001B[0;32m    418\u001B[0m             \u001B[0mpath_or_repo_id\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001B[0m in \u001B[0;36m_inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 118\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    119\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\huggingface_hub\\file_download.py\u001B[0m in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001B[0m\n\u001B[0;32m   1194\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1195\u001B[1;33m                 metadata = get_hf_file_metadata(\n\u001B[0m\u001B[0;32m   1196\u001B[0m                     \u001B[0murl\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001B[0m in \u001B[0;36m_inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 118\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    119\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\huggingface_hub\\file_download.py\u001B[0m in \u001B[0;36mget_hf_file_metadata\u001B[1;34m(url, token, proxies, timeout)\u001B[0m\n\u001B[0;32m   1540\u001B[0m     )\n\u001B[1;32m-> 1541\u001B[1;33m     \u001B[0mhf_raise_for_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1542\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py\u001B[0m in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    274\u001B[0m             )\n\u001B[1;32m--> 275\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mGatedRepoError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    276\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mGatedRepoError\u001B[0m: 401 Client Error. (Request ID: Root=1-6752d3c0-4074b9763769d3625a954932)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/tokenizer_config.json.\nAccess to model meta-llama/Meta-Llama-3-8B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16824/3884528185.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Load the model and tokenizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mmodel_id\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"meta-llama/Meta-Llama-3-8B\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpad_token_id\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meos_token_id\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m model = AutoModelForCausalLM.from_pretrained(\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    641\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    642\u001B[0m         \u001B[1;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 643\u001B[1;33m         \u001B[0mtokenizer_config\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_tokenizer_config\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    644\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m\"_commit_hash\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtokenizer_config\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    645\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"_commit_hash\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer_config\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"_commit_hash\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001B[0m in \u001B[0;36mget_tokenizer_config\u001B[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, **kwargs)\u001B[0m\n\u001B[0;32m    485\u001B[0m     ```\"\"\"\n\u001B[0;32m    486\u001B[0m     \u001B[0mcommit_hash\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"_commit_hash\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 487\u001B[1;33m     resolved_config_file = cached_file(\n\u001B[0m\u001B[0;32m    488\u001B[0m         \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    489\u001B[0m         \u001B[0mTOKENIZER_CONFIG_FILE\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\krzys\\desktop\\pythonproject\\againterpreter\\lib\\site-packages\\transformers\\utils\\hub.py\u001B[0m in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001B[0m\n\u001B[0;32m    431\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    432\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mRepositoryNotFoundError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 433\u001B[1;33m         raise EnvironmentError(\n\u001B[0m\u001B[0;32m    434\u001B[0m             \u001B[1;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    435\u001B[0m             \u001B[1;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: meta-llama/Meta-Llama-3-8B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates:\n",
      " and with probability 0.26030680537223816\n",
      " not with probability 0.13933220505714417\n",
      " here with probability 0.12296022474765778\n",
      " trained with probability 0.058082301169633865\n",
      " please with probability 0.03991934284567833\n",
      "Choosing token  and\n",
      "Candidates:\n",
      " I with probability 0.6940591335296631\n",
      " today with probability 0.053520094603300095\n",
      " here with probability 0.044369716197252274\n",
      " you with probability 0.02864724025130272\n",
      " this with probability 0.02864724025130272\n",
      "Choosing token  today\n",
      "Candidates:\n",
      " I with probability 0.6163644790649414\n",
      " we with probability 0.2569389343261719\n",
      ", with probability 0.07361423969268799\n",
      "'s with probability 0.027081165462732315\n",
      " you with probability 0.008791966363787651\n",
      "Choosing token  I\n",
      "Candidates:\n",
      " will with probability 0.3834528923034668\n",
      "'ll with probability 0.2635430693626404\n",
      "'m with probability 0.14106443524360657\n",
      "'d with probability 0.05189470574259758\n",
      " would with probability 0.031475730240345\n",
      "Choosing token 'll\n",
      "Hello, I'm a language model, and today I'll\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Hello, I'm a language model,\", return_tensors=\"pt\")\n",
    "for i in range(4):\n",
    "    # Get output probabilities\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "    # Look at the top 5 candidates\n",
    "    top_5 = torch.topk(probabilities, 5)\n",
    "    top_5_token_ids = top_5.indices\n",
    "    top_5_probabilities = top_5.values\n",
    "    print(\"Candidates:\")\n",
    "    for token, prob in zip(top_5_token_ids[0], top_5_probabilities[0]):\n",
    "        print(f\"{tokenizer.decode(token.item())} with probability {prob.item()}\")\n",
    "\n",
    "    # Option 1: Choose the most likely candidate\n",
    "    # next_token = top_5_token_ids[0, 0]\n",
    "    # print(f\"Choosing token {tokenizer.decode(next_token.item())}\")\n",
    "\n",
    "    # Option 2: Alternating between first and second candidate\n",
    "    next_token = top_5_token_ids[0, i % 2]\n",
    "    print(f\"Choosing token {tokenizer.decode(next_token.item())}\")\n",
    "\n",
    "    # Add the new token to the input\n",
    "    input_ids = torch.cat([input_ids, next_token[None, None]], dim=-1)\n",
    "\n",
    "print(tokenizer.decode(input_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text -> Binary\n",
    "\n",
    "There are many options, this 5-bit version hacked together for demonstration by my new buddy Sonnet 3.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello World\n",
      "Encoded: 11111010000010101100011000111100000111111011101111100100110000100\n",
      "Decoded: Hello World\n"
     ]
    }
   ],
   "source": [
    "# Define the character set for 5-bit encoding\n",
    "char_set = \" abcdefghijklmnopqrstuvwxyz.,!?\"\n",
    "char_to_bin = {c: f\"{i:05b}\" for i, c in enumerate(char_set)}\n",
    "bin_to_char = {v: k for k, v in char_to_bin.items()}\n",
    "\n",
    "def encode_secret_message(txt):\n",
    "    encoded = \"\"\n",
    "    for c in txt:\n",
    "        if c.lower() in char_set:\n",
    "            if c != c.lower():\n",
    "                encoded += \"11111\"\n",
    "            encoded += char_to_bin[c.lower()]\n",
    "      \n",
    "    return encoded\n",
    "\n",
    "def decode_secret_message(binary_string):\n",
    "    chunks = [binary_string[i:i+5] for i in range(0, len(binary_string), 5)]\n",
    "    decoded = \"\"\n",
    "    uppercase_next = False\n",
    "    for chunk in chunks:\n",
    "        if chunk == \"11111\":\n",
    "            uppercase_next = True\n",
    "        else:\n",
    "            char = bin_to_char.get(chunk, \"\")\n",
    "            if uppercase_next:\n",
    "                char = char.upper()\n",
    "                uppercase_next = False\n",
    "            decoded += char\n",
    "    return decoded\n",
    "\n",
    "t = \"Hello World\"\n",
    "e = encode_secret_message(t)\n",
    "d = decode_secret_message(e)\n",
    "print(f\"Original: {t}\")\n",
    "print(f\"Encoded: {e}\")\n",
    "print(f\"Decoded: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Binary with the choice of token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796cf14f8bb548ecad498866339377fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steganographic text:\n",
      "('When I was young, we used to visit my grandmother in the countryside. She '\n",
      " 'always bakes the most delicious pies. She used a special pie dish that I '\n",
      " 'loved. It was a round, white ceramic pie plate. I loved the shape and the '\n",
      " 'color. I loved the way it looked when she took the pie out of the oven. I '\n",
      " 'loved how the pie crust looked on the plate.\\n'\n",
      " 'I have always wanted to have one. But I never did. I don’t know if I was '\n",
      " \"afraid to ask for it, or I just didn't know how to get\")\n"
     ]
    }
   ],
   "source": [
    "def generate_steganographic_text(prompt, secret_message):\n",
    "    # Encode start prompt and trim to multiple of 5\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids[:, :((input_ids.size(1) // 5) * 5)]\n",
    "    # Turn message into binary string and add tokens to input_ids\n",
    "    encoded = encode_secret_message(secret_message)\n",
    "    for bit in tqdm(encoded):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids).logits\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "        top_5_token_ids = torch.topk(probabilities, 5).indices\n",
    "        next_token = top_5_token_ids[0, 0] if bit == \"0\" else top_5_token_ids[0, 1]\n",
    "        input_ids = torch.cat([input_ids, next_token[None, None]], dim=-1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"When I was young, we used to visit my grandmother in the countryside. \" +\\\n",
    "         \"She always bakes the most delicious pies.\"\n",
    "secret_message = \"Hawk at midnight!\"\n",
    "stego_text = generate_steganographic_text(prompt, secret_message)\n",
    "print(\"Steganographic text:\")\n",
    "pprint(stego_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ddh iHawk at midnigxt!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_steganographic_text(text, start=0):\n",
    "    ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    binary_string = \"\"\n",
    "    with torch.no_grad():\n",
    "            logits = model(ids).logits # << Note: just one forward pass to get all logits\n",
    "    for i in range(start, len(ids[0])):\n",
    "        next_token_logits = logits[:, i-1, :]\n",
    "        top_5_token_ids = torch.topk(next_token_logits, 5).indices\n",
    "        if ids[0][i] == top_5_token_ids[0, 0]:\n",
    "            binary_string += \"0\"\n",
    "        elif ids[0][i] == top_5_token_ids[0, 1]:\n",
    "            binary_string += \"1\"\n",
    "        else: \n",
    "            binary_string += \"0\" # Not part of top5 (happens in prompt) - add 0 to keep length right.\n",
    "    return decode_secret_message(binary_string)\n",
    "\n",
    "decode_steganographic_text(stego_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hawk at midnigxt!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_steganographic_text(stego_text, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d Thanks fer watching!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"In this video about steganography, we are talking to \\\n",
    "a person that is using steganography. We are asking him about \\\n",
    "the process and the reasons for doing it. We are asking about \\\n",
    "his experience and what are the benefits of using steganography.\n",
    "In this video, I’m talking about the process and the reasons for \\\n",
    "doing it. We are talking to someone who is using stegnography and \\\n",
    "asking him about his experience. I’m asking him about the benefits \\\n",
    "and the process.\\nStegano, or steganographic encryption is the practice \\\n",
    "where you embed...\"\"\"\n",
    "decode_steganographic_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11-7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}